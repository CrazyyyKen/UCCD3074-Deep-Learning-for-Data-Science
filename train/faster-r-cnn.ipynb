{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"11237e3a-d27a-4dce-aa90-c49376b661cc","cell_type":"markdown","source":"# 1. Pre-Setup","metadata":{}},{"id":"631c2146","cell_type":"markdown","source":"## Install Packages","metadata":{}},{"id":"b8e949b9","cell_type":"code","source":"!pip install roboflow --quiet\n!pip install pycocotools --quiet\n!pip install albumentations --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dae12b44-94d2-46be-8f7a-1f483428d572","cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"id":"93314ecc-6b9e-4743-a432-bf1edfcc3ffa","cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as T\nimport torch.utils.data\nfrom torch.utils.data import DataLoader\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.datasets import CocoDetection\nfrom torch.utils.data import ConcatDataset\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport json\nimport random\nimport time\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix, precision_recall_fscore_support, ConfusionMatrixDisplay\n\nfrom roboflow import Roboflow\n\nimport matplotlib.patches as patches","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"182acbba","cell_type":"markdown","source":"## Download Dataset from Roboflow","metadata":{}},{"id":"a441f7e6","cell_type":"code","source":"# Download the dataset\nAPI_KEY = \"4PCfwxj1tbPm25eqqLhX\"\n\nrf = Roboflow(api_key=API_KEY)\nproject = rf.workspace(\"dent-ydn9e\").project(\"defect-detection-rhju6\")\nversion = project.version(1)\ndataset = version.download(\"coco\")\ndata_dir = dataset.location\n\nprint(\"Downloaded to:\", data_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"88d8ced6-b899-4f85-82ac-62a6a7f4ab0b","cell_type":"markdown","source":"# 2. Hyperparameters and Configs","metadata":{}},{"id":"582c2327-cc47-4648-a19a-b949572be190","cell_type":"markdown","source":"## Dataset Configs","metadata":{}},{"id":"94cfc61e-0282-4fd5-a69c-de011d443e17","cell_type":"code","source":"# Set dataset paths\ntrain_dir = os.path.join(data_dir, \"train\")\nvalid_dir = os.path.join(data_dir, \"valid\")\ntest_dir  = os.path.join(data_dir, \"test\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1ea9fc80-6a7d-4b22-aaf3-b0daf199dddd","cell_type":"markdown","source":"## Augmentation Hyperparameters","metadata":{}},{"id":"3f31cad6-ec6f-4766-a0c3-6cbf0fc0ddf2","cell_type":"code","source":"augmentation_hyperparameters = {\n    'resize':  { 'height': 512, 'width': 512 },\n    'jitter': dict(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02, p=0.5),\n    'rotate': dict(limit=10, p=0.5, border_mode=0),\n    'hflip': dict(p=0.5),\n    'vflip': dict(p=0.5),\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0d3672b5-0b70-4db8-95ae-abbf6a194ac6","cell_type":"markdown","source":"## Dataset Preparation Hyperparameters","metadata":{}},{"id":"7b8f2d8b-b327-4cb3-8855-51755b2d9994","cell_type":"code","source":"batch_size_train = 4\nbatch_size_valid = 2\nbatch_size_test = 2\n\n# Define valid classes to detect: Only use these valid classes (ignore others)\nVALID_CLASSES = {\n    \"dent_marginal\": 1,\n    \"dent_unacceptable\": 2\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9f600ac3-4060-4fd5-9258-938f7aa7fb69","cell_type":"markdown","source":"## Model Training Hyperparameters ","metadata":{}},{"id":"c02e0e2e-68c6-410c-959a-b081d1d42fe0","cell_type":"code","source":"learning_rate = 0.005\nmomentum = 0.9\nweight_decay = 0.0005\n\nstep_size = 3\ngamma = 0.1\n\nnum_epochs = 10\n\niou_threshold = 0.5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0456084d-dffd-4958-8567-408e4b177db8","cell_type":"markdown","source":"## Visualization Hyperparameters","metadata":{}},{"id":"759661cc-aac5-443e-bc0b-468022281dbe","cell_type":"code","source":"score_threshold = 0.5  # For visualization threshold","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"496110f9-09d7-4ac6-a4f9-d3d5cab70c82","cell_type":"markdown","source":"# 3. Data Preparation","metadata":{}},{"id":"983bd7f8-3390-437a-9d6b-727a848506a3","cell_type":"markdown","source":"## Define Dataset Class","metadata":{}},{"id":"556826ef","cell_type":"code","source":"class DefectDetectionDataset(CocoDetection):\n    def __init__(self, img_folder, ann_file, transforms=None):\n        super().__init__(img_folder, ann_file)\n        self._transforms = transforms\n        self.valid_class_ids = set(VALID_CLASSES.values())\n\n        # Precompute valid IDs\n        self.valid_ids = []\n        for idx in range(len(self.ids)):\n            anns = self.coco.loadAnns(self.ids[idx])\n            if any(ann['category_id'] in self.valid_class_ids for ann in anns):\n                self.valid_ids.append(self.ids[idx])\n\n        self.ids = self.valid_ids\n\n    def __getitem__(self, idx):\n        img, _ = super().__getitem__(idx)\n        img = np.array(img)  # Convert PIL image to numpy array for Albumentations\n        h, w = img.shape[:2]\n\n        ann_ids = self.coco.getAnnIds(imgIds=self.ids[idx])\n        anns = self.coco.loadAnns(ann_ids)\n\n        boxes, labels = [], []\n        for ann in anns:\n            if ann['category_id'] in self.valid_class_ids:\n                xmin, ymin, width, height = ann['bbox']\n                # Convert to absolute coordinates and clip to image dimensions\n                xmin = max(0, min(xmin, w-1))\n                ymin = max(0, min(ymin, h-1))\n                xmax = max(0, min(xmin + width, w-1))\n                ymax = max(0, min(ymin + height, h-1))\n                if xmax > xmin and ymax > ymin:  # Only keep valid boxes\n                    boxes.append([xmin, ymin, xmax, ymax])\n                    labels.append(ann['category_id'])\n\n        if self._transforms:\n            try:\n                transformed = self._transforms(\n                    image=img,\n                    bboxes=boxes,\n                    labels=labels\n                )\n                img = transformed['image']\n                boxes = transformed['bboxes']\n                labels = transformed['labels']\n            except Exception as e:\n                print(f\"Transform failed for image {self.ids[idx]}: {e}\")\n                # Fallback to just transforming the image\n                img = self._transforms(image=img)['image']\n                boxes = []\n                labels = []\n\n        # Convert to tensors\n        boxes = torch.as_tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4))\n        labels = torch.as_tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64)\n\n        target = {\n            'boxes': boxes,\n            'labels': labels,\n            'image_id': torch.tensor([self.ids[idx]])\n        }\n\n        return img, target","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5098f443-c81a-467d-be4b-d91ce42023df","cell_type":"markdown","source":"## Transform Function","metadata":{}},{"id":"7da3aaea-dafe-467c-8704-a1de91a59f99","cell_type":"markdown","source":"### Calculate Mean and Std of Dataset","metadata":{}},{"id":"e1f3a5aa-cb46-42b6-a06f-d91c264a00b1","cell_type":"code","source":"def calculate_dataset_stats(dataset, num_samples=None):\n    \"\"\"\n    Calculate mean and std of dataset images\n    Args:\n        dataset: PyTorch dataset\n        num_samples: Number of samples to use (None for all)\n    \"\"\"\n    if num_samples is None:\n        num_samples = len(dataset)\n    else:\n        num_samples = min(num_samples, len(dataset))\n\n    # Initialize accumulators\n    pixel_sum = torch.zeros(3)\n    pixel_sq_sum = torch.zeros(3)\n    num_pixels = 0\n\n    # Use random samples for efficiency\n    indices = torch.randperm(len(dataset))[:num_samples]\n\n    for idx in tqdm(indices, desc=\"Calculating stats\"):\n        img, _ = dataset[int(idx)]\n\n        if isinstance(img, np.ndarray):\n            img = torch.from_numpy(img).permute(2, 0, 1)  # HWC to CHW\n        elif isinstance(img, torch.Tensor):\n            if img.ndim == 3 and img.shape[0] != 3:  # If not CHW\n                img = img.permute(2, 0, 1)\n\n        img = img.float() / 255.0  # Normalize to [0,1]\n\n        pixel_sum += img.sum(dim=[1, 2])\n        pixel_sq_sum += (img**2).sum(dim=[1, 2])\n        num_pixels += img.shape[1] * img.shape[2]\n\n    mean = pixel_sum / num_pixels\n    std = torch.sqrt((pixel_sq_sum / num_pixels) - mean**2)\n\n    return mean.tolist(), std.tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9877b503-01bf-475e-b9cf-75983c0c56d1","cell_type":"code","source":"temp_dataset = DefectDetectionDataset(train_dir, os.path.join(train_dir, \"_annotations.coco.json\"), transforms=None)\nmean, std = calculate_dataset_stats(temp_dataset)\n\nprint(f\"Calculated mean: {mean}\")\nprint(f\"Calculated std: {std}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"64f11bc5-1c34-4055-b9ac-e69f7d08188a","cell_type":"code","source":"def get_transform(aug=True):\n    if aug:\n        # Training pipeline: resize → augment → normalize → tensor\n        return A.Compose(\n            [\n                A.Resize(**augmentation_hyperparameters['resize']),\n                A.ColorJitter(**augmentation_hyperparameters['jitter']),\n                A.Rotate(**augmentation_hyperparameters['rotate']),\n                A.HorizontalFlip(**augmentation_hyperparameters['hflip']),\n                A.VerticalFlip(**augmentation_hyperparameters['vflip']),\n                A.Normalize(mean=mean, std=std),\n                ToTensorV2(),\n            ],\n            bbox_params=A.BboxParams(\n                format='pascal_voc',\n                label_fields=['labels'],\n                clip=True,\n                min_visibility=0.3\n            )\n        )\n    \n    # Validation pipeline: resize → normalize → tensor\n    return A.Compose(\n        [\n            A.Resize(**augmentation_hyperparameters['resize']),\n            A.Normalize(mean=mean, std=std),\n            ToTensorV2(),\n        ],\n        bbox_params=A.BboxParams(\n            format='pascal_voc',\n            label_fields=['labels'],\n            clip=True,\n            min_visibility=0.3\n        )\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7fe0003b-0495-492a-aec6-1e1082ec1b23","cell_type":"markdown","source":"## Load Datasets and DataLoaders","metadata":{}},{"id":"70c4520b-199e-422a-b67b-31384c4b2184","cell_type":"code","source":"# Load datasets\nuse_augmentation = True  # True to apply augmentation on train dataset, otherwise false\n\nif use_augmentation:\n    orig_ds = DefectDetectionDataset(\n        train_dir,\n        os.path.join(train_dir, \"_annotations.coco.json\"),\n        transforms=get_transform(aug=False)\n    )\n    aug_ds = DefectDetectionDataset(\n        train_dir,\n        os.path.join(train_dir, \"_annotations.coco.json\"),\n        transforms=get_transform(aug=True)\n    )\n    train_dataset = ConcatDataset([orig_ds, aug_ds])\nelse:\n    train_dataset = DefectDetectionDataset(\n        train_dir,\n        os.path.join(train_dir, \"_annotations.coco.json\"),\n        transforms=get_transform(aug=False)\n    )\n\nvalid_dataset = DefectDetectionDataset(valid_dir, os.path.join(valid_dir, \"_annotations.coco.json\"), transforms=get_transform(aug=False))\ntest_dataset  = DefectDetectionDataset(test_dir,  os.path.join(test_dir, \"_annotations.coco.json\"), transforms=get_transform(aug=False))\n\n# Dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size_valid, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n\nprint(f\"Train Samples: {len(train_dataset)}, Valid Samples: {len(valid_dataset)}, Test Samples: {len(test_dataset)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"502db25f-28f5-4859-a5d9-69a0ef42b407","cell_type":"markdown","source":"# 4. Training and Evaluation Functions","metadata":{}},{"id":"e7087fb9","cell_type":"markdown","source":"## Training Function","metadata":{}},{"id":"22a942a4-c137-48a2-bff1-d86787d58998","cell_type":"code","source":"def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50):\n    model.train()\n    running_loss = 0.0\n\n    for batch_idx, (images, targets) in enumerate(data_loader):\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        running_loss += losses.item()\n\n        if (batch_idx + 1) % print_freq == 0:\n            print(f\"[Epoch {epoch+1}] [Batch {batch_idx+1}/{len(data_loader)}] Training...\")\n\n    return running_loss / len(data_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"65807c52-2915-4315-b05d-2a7c0e67012e","cell_type":"markdown","source":"## Evaluation Function","metadata":{}},{"id":"b82f1376","cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, data_loader, device, epoch=None, print_freq=50):\n    model.train()\n    running_loss = 0.0\n\n    for batch_idx, (images, targets) in enumerate(data_loader):\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        running_loss += losses.item()\n\n        if (batch_idx + 1) % print_freq == 0:\n            if epoch is not None:\n                print(f\"[Epoch {epoch+1}] [Batch {batch_idx+1}/{len(data_loader)}] Validating...\")\n\n    return running_loss / len(data_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ec852e5f-b295-49c1-b151-d8db445aefac","cell_type":"markdown","source":"## Calculate IOU Function","metadata":{}},{"id":"e8007dd2-bf5d-46f4-b5cb-797e01758732","cell_type":"code","source":"def calculate_iou(box1, box2):\n    x1, y1 = max(box1[0], box2[0]), max(box1[1], box2[1])\n    x2, y2 = min(box1[2], box2[2]), min(box1[3], box2[3])\n    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n\n    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n\n    union_area = box1_area + box2_area - inter_area\n    if union_area == 0:\n        return 0\n    return inter_area / union_area","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bca834c9-17f8-4218-9aeb-6a64724c013a","cell_type":"markdown","source":"## Calculate Precision-Recall Function","metadata":{}},{"id":"ca2f89c3-647f-4bb3-ae36-ac5c57335db4","cell_type":"code","source":"def calculate_precision_recall(scores, matches):\n    if len(scores) == 0:\n        return np.array([0]), np.array([0])\n\n    scores = np.array(scores)\n    matches = np.array(matches)\n\n    indices = np.argsort(-scores)\n    matches = matches[indices]\n\n    tp = np.cumsum(matches)\n    fp = np.cumsum(1 - matches)\n\n    precision = tp / (tp + fp + 1e-6)\n    recall = tp / (tp[-1] + 1e-6)\n\n    return precision, recall","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8cf3f43f-305a-4c98-aab7-6c99beeac16e","cell_type":"markdown","source":"## Calculate mAP Function","metadata":{}},{"id":"f28edd6d-a1f3-413b-9341-202236081ac9","cell_type":"code","source":"def calculate_map(scores, matches):\n    precision, recall = calculate_precision_recall(scores, matches)\n    ap = 0\n    for t in np.linspace(0, 1, 11):\n        p = precision[recall >= t]\n        if p.size > 0:\n            ap += np.max(p)\n    return ap / 11","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"197164e5-70a7-45b1-b4e8-3c58cfaa6357","cell_type":"markdown","source":"## Compute mAP and PR Curves Function","metadata":{}},{"id":"4ba4fe57-5072-4bea-af39-5d9f45c11bb8","cell_type":"code","source":"@torch.no_grad()\ndef compute_map_and_pr_curves(model, data_loader, device, iou_threshold=iou_threshold):\n    model.eval()\n\n    all_scores = []\n    all_matches = []\n\n    per_class_scores = {c: [] for c in VALID_CLASSES.values()}\n    per_class_matches = {c: [] for c in VALID_CLASSES.values()}\n\n    for images, targets in data_loader:\n        images = list(img.to(device) for img in images)\n        outputs = model(images)\n\n        for output, target in zip(outputs, targets):\n            pred_boxes = output['boxes'].cpu().numpy()\n            pred_scores = output['scores'].cpu().numpy()\n            pred_labels = output['labels'].cpu().numpy()\n\n            gt_boxes = target['boxes'].numpy()\n            gt_labels = target['labels'].numpy()\n\n            keep = pred_scores >= 0.05\n            pred_boxes = pred_boxes[keep]\n            pred_scores = pred_scores[keep]\n            pred_labels = pred_labels[keep]\n\n            matched = np.zeros(len(pred_boxes))\n\n            for gt_idx, gt_box in enumerate(gt_boxes):\n                best_iou, best_idx = 0, -1\n                for pred_idx, pred_box in enumerate(pred_boxes):\n                    iou = calculate_iou(gt_box, pred_box)\n                    if iou > best_iou:\n                        best_iou = iou\n                        best_idx = pred_idx\n\n                if best_iou >= iou_threshold:\n                    matched[best_idx] = 1\n\n            all_scores.extend(pred_scores.tolist())\n            all_matches.extend(matched.tolist())\n\n            for c in VALID_CLASSES.values():\n                gt_class_boxes = gt_boxes[gt_labels == c]\n                pred_class_boxes = pred_boxes[pred_labels == c]\n                pred_class_scores = pred_scores[pred_labels == c]\n\n                matched_c = np.zeros(len(pred_class_boxes))\n\n                for gt_idx, gt_box in enumerate(gt_class_boxes):\n                    best_iou_c, best_idx_c = 0, -1\n                    for pred_idx, pred_box in enumerate(pred_class_boxes):\n                        iou = calculate_iou(gt_box, pred_box)\n                        if iou > best_iou_c:\n                            best_iou_c = iou\n                            best_idx_c = pred_idx\n\n                    if best_iou_c >= iou_threshold:\n                        if best_idx_c != -1:\n                            matched_c[best_idx_c] = 1\n\n                per_class_scores[c].extend(pred_class_scores.tolist())\n                per_class_matches[c].extend(matched_c.tolist())\n\n    # Global\n    global_precision, global_recall = calculate_precision_recall(all_scores, all_matches)\n    global_map = calculate_map(all_scores, all_matches)\n\n    # Per Class\n    per_class_pr = {}\n    per_class_map = {}\n    for c in VALID_CLASSES.values():\n        p, r = calculate_precision_recall(per_class_scores[c], per_class_matches[c])\n        per_class_pr[c] = (p, r)\n        per_class_map[c] = calculate_map(per_class_scores[c], per_class_matches[c])\n\n    return global_map, per_class_map, global_precision, global_recall, per_class_pr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d392e622-b168-4d15-8185-5826e8e0eeb6","cell_type":"markdown","source":"# 5. Model Training","metadata":{}},{"id":"fa51bf3a-48f5-4d01-8190-ba1e8a9f3184","cell_type":"markdown","source":"## Model Preparation","metadata":{}},{"id":"f1892566-187d-4692-a80e-42fa500e2e03","cell_type":"code","source":"def get_model(num_classes):\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel = get_model(num_classes=3)  # 2 classes + background\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"42d05716-b018-4f23-acd7-4625e32e864e","cell_type":"markdown","source":"## Training Loop","metadata":{}},{"id":"a637e57b-fdce-4438-8d03-2a2413ef0a50","cell_type":"code","source":"start_epoch = 0\nbest_map = 0.0\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n\ntrain_losses = []\nvalid_losses = []\nmaps = []\nper_class_maps = {c: [] for c in VALID_CLASSES.values()}\n\ncheckpoint_path = \"checkpoint.pth\"\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    start_epoch = checkpoint['epoch'] + 1\n    best_map = checkpoint['best_map']\n    model.train()\n    print(f\"Loaded checkpoint from epoch {start_epoch-1}, best validation mAP so far: {best_map:.4f}\")\nelse:\n    print(\"No checkpoint found, starting from scratch.\")\n\nfor epoch in range(start_epoch, num_epochs):\n    start_time = time.time()\n\n    train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n    valid_loss = evaluate(model, valid_loader, device, epoch)\n\n    global_map, per_class_map, global_precision, global_recall, per_class_pr = compute_map_and_pr_curves(model, valid_loader, device)\n\n    lr_scheduler.step()\n\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    maps.append(global_map)\n    for c in VALID_CLASSES.values():\n        per_class_maps[c].append(per_class_map[c])\n\n    elapsed = time.time() - start_time\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}] Completed:\")\n    print(f\"    Train Loss        : {train_loss:.4f}\")\n    print(f\"    Valid Loss        : {valid_loss:.4f}\")\n    print(f\"    Validation mAP@0.5: {global_map:.4f}\")\n    print(f\"    Time Elapsed      : {elapsed/60:.2f} min\")\n    print(\"-\" * 50)\n\n    # Save best model\n    if global_map > best_map:\n        best_map = global_map\n        torch.save(model.state_dict(), \"best_fasterrcnn_defect_detection.pth\")\n        print(f\"Best model saved with mAP {best_map:.4f}\")\n\n    # Save full checkpoint\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': lr_scheduler.state_dict(),\n        'best_map': best_map\n    }, f'checkpoint_epoch{epoch+1}.pth')\n    print(f\"Checkpoint saved at epoch {epoch+1}\")\n\nprint(\"Training Complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"fa999eb3-1c6d-4ff8-86db-e6faf0ffab08","cell_type":"markdown","source":"# 6. Metrics Saving and Visualizations","metadata":{}},{"id":"2fd45480-b569-4c25-a078-2e5f4305918c","cell_type":"markdown","source":"## Save Training Results","metadata":{}},{"id":"bef48c72-89cc-4f3f-9425-3fcb2db46484","cell_type":"code","source":"# Save training metrics to CSV\nmetrics_df = pd.DataFrame({\n    \"Epoch\": list(range(1, num_epochs+1)),\n    \"Train Loss\": train_losses,\n    \"Valid Loss\": valid_losses,\n    \"mAP@0.5\": maps\n})\nmetrics_df.to_csv(\"training_metrics.csv\", index=False)\nprint(\"Saved training metrics to training_metrics.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"fd26c918-f9e6-4878-89d9-1c4cec75037b","cell_type":"markdown","source":"## Plot Loss Curves","metadata":{}},{"id":"fcb8a25e-8bd6-48d0-96b2-3b977a6bdda3","cell_type":"code","source":"# Plot Loss Curves\nplt.figure()\nplt.plot(train_losses, label=\"Train Loss\")\nplt.plot(valid_losses, label=\"Valid Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Train and Validation Loss\")\nplt.grid(True)\nplt.savefig(\"loss_curve.png\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"73314002-29e9-481a-ba0f-44365c23a23f","cell_type":"markdown","source":"## Plot Overall mAP Curve","metadata":{}},{"id":"65b6c17a-903e-4da3-8830-692a2f9a2dea","cell_type":"code","source":"# Plot Overall mAP\nplt.figure()\nplt.plot(maps, label=\"Validation mAP@0.5\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"mAP@0.5\")\nplt.legend()\nplt.title(\"Validation mAP@0.5 Curve\")\nplt.grid(True)\nplt.savefig(\"map_curve.png\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"337c0ebb-4682-4020-806a-826b0f33e7af","cell_type":"markdown","source":"## Plot Per-Class mAP Curve","metadata":{}},{"id":"dff0755d-656a-48df-92ff-a58a1769b920","cell_type":"code","source":"# Plot Per-Class mAP\nplt.figure()\nfor c, values in per_class_maps.items():\n    class_name = [k for k,v in VALID_CLASSES.items() if v == c][0]\n    plt.plot(values, label=f\"{class_name}\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Per-Class mAP@0.5\")\nplt.legend()\nplt.title(\"Per-Class Validation mAP@0.5\")\nplt.grid(True)\nplt.savefig(\"per_class_map_curve.png\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"003d0c23-646c-4178-8604-28bdffbbccde","cell_type":"markdown","source":"## Plot PR Curves","metadata":{}},{"id":"bca0ace2-4069-4414-89c9-e785f6bba010","cell_type":"code","source":"def plot_pr_curve(precision, recall, title, filename):\n    plt.figure()\n    plt.plot(recall, precision, marker='.')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title(title)\n    plt.grid(True)\n    plt.savefig(filename)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"339e6d94-6cc9-4e17-b179-30cebe544da5","cell_type":"code","source":"# Plot Global PR Curve\nplot_pr_curve(global_precision, global_recall, \"Global PR Curve\", \"global_pr_curve.png\")\n\n# Plot Per-Class PR Curves\nfor class_id, (precision_c, recall_c) in per_class_pr.items():\n    class_name = [k for k,v in VALID_CLASSES.items() if v == class_id][0]\n    plot_pr_curve(precision_c, recall_c, f\"{class_name} PR Curve\", f\"{class_name.lower()}_pr_curve.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ce474f7d-e323-4d30-8c8e-ea781fae4b0b","cell_type":"markdown","source":"## Visualize Predictions","metadata":{}},{"id":"95ae11bc-0440-4d2c-b939-4bb0b689201f","cell_type":"code","source":"# Visualize Predictions\n\nmodel.eval()\nfor i in range(5):\n    img, _ = test_dataset[i]\n    img = img.to(device)\n    with torch.no_grad():\n        prediction = model([img])\n\n    img = img.permute(1,2,0).cpu().numpy()\n\n    fig, ax = plt.subplots(1, 1, figsize=(12,9))\n    ax.imshow(img)\n    boxes = prediction[0]['boxes'].cpu().numpy()\n    scores = prediction[0]['scores'].cpu().numpy()\n    labels = prediction[0]['labels'].cpu().numpy()\n\n    for box, score, label in zip(boxes, scores, labels):\n        if score >= score_threshold:\n            xmin, ymin, xmax, ymax = box\n            width, height = xmax - xmin, ymax - ymin\n            rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='r', facecolor='none')\n            ax.add_patch(rect)\n            label_name = [k for k,v in VALID_CLASSES.items() if v == label][0]\n            ax.text(xmin, ymin, f\"{label_name}: {score:.2f}\", color='white', fontsize=12, backgroundcolor='red')\n\n    plt.axis('off')\n    plt.savefig(f\"prediction_{i}.png\")\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}