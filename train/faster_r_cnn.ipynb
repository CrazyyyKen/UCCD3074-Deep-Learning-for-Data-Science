{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "11237e3a-d27a-4dce-aa90-c49376b661cc",
      "cell_type": "markdown",
      "source": [
        "# 1. Pre-Setup"
      ],
      "metadata": {
        "id": "11237e3a-d27a-4dce-aa90-c49376b661cc"
      }
    },
    {
      "id": "631c2146",
      "cell_type": "markdown",
      "source": [
        "## Install Packages"
      ],
      "metadata": {
        "id": "631c2146"
      }
    },
    {
      "id": "b8e949b9",
      "cell_type": "code",
      "source": [
        "!pip install roboflow --quiet\n",
        "!pip install pycocotools --quiet\n",
        "!pip install albumentations --quiet"
      ],
      "metadata": {
        "trusted": true,
        "id": "b8e949b9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "dae12b44-94d2-46be-8f7a-1f483428d572",
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "dae12b44-94d2-46be-8f7a-1f483428d572"
      }
    },
    {
      "id": "93314ecc-6b9e-4743-a432-bf1edfcc3ffa",
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torch.utils.data\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.datasets import CocoDetection\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, ConfusionMatrixDisplay\n",
        "from roboflow import Roboflow\n",
        "import matplotlib.patches as patches"
      ],
      "metadata": {
        "trusted": true,
        "id": "93314ecc-6b9e-4743-a432-bf1edfcc3ffa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "182acbba",
      "cell_type": "markdown",
      "source": [
        "## Download Dataset from Roboflow"
      ],
      "metadata": {
        "id": "182acbba"
      }
    },
    {
      "id": "a441f7e6",
      "cell_type": "code",
      "source": [
        "API_KEY = \"4PCfwxj1tbPm25eqqLhX\"\n",
        "\n",
        "rf = Roboflow(api_key=API_KEY)\n",
        "project = rf.workspace(\"dent-ydn9e\").project(\"defect-detection-rhju6\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"coco\")\n",
        "data_dir = dataset.location\n",
        "\n",
        "print(\"Downloaded to:\", data_dir)"
      ],
      "metadata": {
        "trusted": true,
        "id": "a441f7e6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "88d8ced6-b899-4f85-82ac-62a6a7f4ab0b",
      "cell_type": "markdown",
      "source": [
        "# 2. Hyperparameters and Configs"
      ],
      "metadata": {
        "id": "88d8ced6-b899-4f85-82ac-62a6a7f4ab0b"
      }
    },
    {
      "id": "582c2327-cc47-4648-a19a-b949572be190",
      "cell_type": "markdown",
      "source": [
        "## Dataset Configs"
      ],
      "metadata": {
        "id": "582c2327-cc47-4648-a19a-b949572be190"
      }
    },
    {
      "id": "94cfc61e-0282-4fd5-a69c-de011d443e17",
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join(data_dir, \"train\")\n",
        "valid_dir = os.path.join(data_dir, \"valid\")\n",
        "test_dir  = os.path.join(data_dir, \"test\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "94cfc61e-0282-4fd5-a69c-de011d443e17"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1ea9fc80-6a7d-4b22-aaf3-b0daf199dddd",
      "cell_type": "markdown",
      "source": [
        "## Augmentation Hyperparameters"
      ],
      "metadata": {
        "id": "1ea9fc80-6a7d-4b22-aaf3-b0daf199dddd"
      }
    },
    {
      "id": "3f31cad6-ec6f-4766-a0c3-6cbf0fc0ddf2",
      "cell_type": "code",
      "source": [
        "augmentation_hyperparameters = {\n",
        "    'resize':  { 'height': 512, 'width': 512 },\n",
        "    'jitter': dict(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02, p=0.5),\n",
        "    'rotate': dict(limit=10, p=0.5, border_mode=0),\n",
        "    'hflip': dict(p=0.5),\n",
        "    'vflip': dict(p=0.5),\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "id": "3f31cad6-ec6f-4766-a0c3-6cbf0fc0ddf2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0d3672b5-0b70-4db8-95ae-abbf6a194ac6",
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation Hyperparameters"
      ],
      "metadata": {
        "id": "0d3672b5-0b70-4db8-95ae-abbf6a194ac6"
      }
    },
    {
      "id": "7b8f2d8b-b327-4cb3-8855-51755b2d9994",
      "cell_type": "code",
      "source": [
        "batch_size_train = 4\n",
        "batch_size_valid = 2\n",
        "batch_size_test = 2\n",
        "\n",
        "# Define valid classes to detect: Only use these valid classes (ignore others)\n",
        "VALID_CLASSES = {\n",
        "    \"dent_marginal\": 1,\n",
        "    \"dent_unacceptable\": 2\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "id": "7b8f2d8b-b327-4cb3-8855-51755b2d9994"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9f600ac3-4060-4fd5-9258-938f7aa7fb69",
      "cell_type": "markdown",
      "source": [
        "## Model Training Hyperparameters"
      ],
      "metadata": {
        "id": "9f600ac3-4060-4fd5-9258-938f7aa7fb69"
      }
    },
    {
      "id": "c02e0e2e-68c6-410c-959a-b081d1d42fe0",
      "cell_type": "code",
      "source": [
        "learning_rate = 0.005\n",
        "momentum = 0.9\n",
        "weight_decay = 0.0005\n",
        "\n",
        "step_size = 3\n",
        "gamma = 0.1\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "iou_threshold = 0.5"
      ],
      "metadata": {
        "trusted": true,
        "id": "c02e0e2e-68c6-410c-959a-b081d1d42fe0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0456084d-dffd-4958-8567-408e4b177db8",
      "cell_type": "markdown",
      "source": [
        "## Visualization Hyperparameters"
      ],
      "metadata": {
        "id": "0456084d-dffd-4958-8567-408e4b177db8"
      }
    },
    {
      "id": "759661cc-aac5-443e-bc0b-468022281dbe",
      "cell_type": "code",
      "source": [
        "score_threshold = 0.5"
      ],
      "metadata": {
        "trusted": true,
        "id": "759661cc-aac5-443e-bc0b-468022281dbe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "496110f9-09d7-4ac6-a4f9-d3d5cab70c82",
      "cell_type": "markdown",
      "source": [
        "# 3. Data Preparation"
      ],
      "metadata": {
        "id": "496110f9-09d7-4ac6-a4f9-d3d5cab70c82"
      }
    },
    {
      "id": "983bd7f8-3390-437a-9d6b-727a848506a3",
      "cell_type": "markdown",
      "source": [
        "## Define Dataset Class"
      ],
      "metadata": {
        "id": "983bd7f8-3390-437a-9d6b-727a848506a3"
      }
    },
    {
      "id": "556826ef",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Lee Zhen Ting\n",
        "########################################\n",
        "'''\n",
        "\n",
        "class DefectDetectionDataset(CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, transforms=None):\n",
        "        \"\"\"\n",
        "        Custom dataset for defect detection using COCO format annotations.\n",
        "        Filters out images without relevant class annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__(img_folder, ann_file)\n",
        "        self._transforms = transforms\n",
        "        self.valid_class_ids = set(VALID_CLASSES.values())\n",
        "\n",
        "        # Filter out image IDs that do not contain any valid annotations\n",
        "        self.valid_ids = []\n",
        "        for idx in range(len(self.ids)):\n",
        "            anns = self.coco.loadAnns(self.ids[idx])\n",
        "            if any(ann['category_id'] in self.valid_class_ids for ann in anns):\n",
        "                self.valid_ids.append(self.ids[idx])\n",
        "\n",
        "        self.ids = self.valid_ids\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, _ = super().__getitem__(idx)\n",
        "\n",
        "        # Convert PIL image to numpy array for Albumentations\n",
        "        img = np.array(img)\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=self.ids[idx])\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        boxes, labels = [], []\n",
        "        for ann in anns:\n",
        "            if ann['category_id'] in self.valid_class_ids:\n",
        "                xmin, ymin, width, height = ann['bbox']\n",
        "\n",
        "                # Convert COCO bbox (xmin, ymin, width, height) to (xmin, ymin, xmax, ymax)\n",
        "                xmin = max(0, min(xmin, w-1))\n",
        "                ymin = max(0, min(ymin, h-1))\n",
        "                xmax = max(0, min(xmin + width, w-1))\n",
        "                ymax = max(0, min(ymin + height, h-1))\n",
        "\n",
        "                # Keep only valid bounding boxes\n",
        "                if xmax > xmin and ymax > ymin:\n",
        "                    boxes.append([xmin, ymin, xmax, ymax])\n",
        "                    labels.append(ann['category_id'])\n",
        "\n",
        "        # Apply Albumentations transformations if available\n",
        "        if self._transforms:\n",
        "            try:\n",
        "                transformed = self._transforms(\n",
        "                    image=img,\n",
        "                    bboxes=boxes,\n",
        "                    labels=labels\n",
        "                )\n",
        "                img = transformed['image']\n",
        "                boxes = transformed['bboxes']\n",
        "                labels = transformed['labels']\n",
        "            except Exception as e:\n",
        "\n",
        "                # Handle transformation failures gracefully\n",
        "                print(f\"Transform failed for image {self.ids[idx]}: {e}\")\n",
        "                img = self._transforms(image=img)['image']\n",
        "                boxes = []\n",
        "                labels = []\n",
        "\n",
        "        # Convert boxes and labels to tensors\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4))\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "        # Prepare target dictionary for the model\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'image_id': torch.tensor([self.ids[idx]])\n",
        "        }\n",
        "\n",
        "        return img, target"
      ],
      "metadata": {
        "trusted": true,
        "id": "556826ef"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5098f443-c81a-467d-be4b-d91ce42023df",
      "cell_type": "markdown",
      "source": [
        "## Transform Dataset"
      ],
      "metadata": {
        "id": "5098f443-c81a-467d-be4b-d91ce42023df"
      }
    },
    {
      "id": "7da3aaea-dafe-467c-8704-a1de91a59f99",
      "cell_type": "markdown",
      "source": [
        "### Calculate Mean and Std Function"
      ],
      "metadata": {
        "id": "7da3aaea-dafe-467c-8704-a1de91a59f99"
      }
    },
    {
      "id": "e1f3a5aa-cb46-42b6-a06f-d91c264a00b1",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Lee Zhen Ting\n",
        "########################################\n",
        "'''\n",
        "\n",
        "def calculate_dataset_stats(dataset, num_samples=None):\n",
        "    \"\"\"\n",
        "    Calculates the mean and standard deviation (per channel) of a dataset's images.\n",
        "\n",
        "    Args:\n",
        "        dataset: A PyTorch-style dataset that returns images (in CHW or HWC format).\n",
        "        num_samples: Number of samples to use for estimation. If None, use the entire dataset.\n",
        "\n",
        "    Returns:\n",
        "        mean (list): List of mean values for each channel [R, G, B].\n",
        "        std (list): List of standard deviation values for each channel [R, G, B].\n",
        "    \"\"\"\n",
        "\n",
        "    if num_samples is None:\n",
        "        num_samples = len(dataset)\n",
        "    else:\n",
        "        num_samples = min(num_samples, len(dataset))\n",
        "\n",
        "    # Initialize accumulators for sum and squared sum per channel\n",
        "    pixel_sum = torch.zeros(3)\n",
        "    pixel_sq_sum = torch.zeros(3)\n",
        "    num_pixels = 0\n",
        "\n",
        "    # Randomly select indices to estimate statistics efficiently\n",
        "    indices = torch.randperm(len(dataset))[:num_samples]\n",
        "\n",
        "    for idx in tqdm(indices, desc=\"Calculating stats\"):\n",
        "        img, _ = dataset[int(idx)]\n",
        "\n",
        "        # Convert image to torch tensor in CHW format if needed\n",
        "        if isinstance(img, np.ndarray):\n",
        "            img = torch.from_numpy(img).permute(2, 0, 1)\n",
        "        elif isinstance(img, torch.Tensor):\n",
        "            if img.ndim == 3 and img.shape[0] != 3:\n",
        "                img = img.permute(2, 0, 1)\n",
        "\n",
        "        # Normalize pixel values to [0, 1]\n",
        "        img = img.float() / 255.0\n",
        "\n",
        "        # Accumulate pixel-wise sum and squared sum per channel\n",
        "        pixel_sum += img.sum(dim=[1, 2])\n",
        "        pixel_sq_sum += (img**2).sum(dim=[1, 2])\n",
        "        num_pixels += img.shape[1] * img.shape[2]\n",
        "\n",
        "    # Compute mean and std per channel\n",
        "    mean = pixel_sum / num_pixels\n",
        "    std = torch.sqrt((pixel_sq_sum / num_pixels) - mean**2)\n",
        "\n",
        "    return mean.tolist(), std.tolist()"
      ],
      "metadata": {
        "trusted": true,
        "id": "e1f3a5aa-cb46-42b6-a06f-d91c264a00b1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Mean and Std of Dataset"
      ],
      "metadata": {
        "id": "qI4lCgLtqzI_"
      },
      "id": "qI4lCgLtqzI_"
    },
    {
      "id": "9877b503-01bf-475e-b9cf-75983c0c56d1",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Lee Zhen Ting\n",
        "########################################\n",
        "'''\n",
        "\n",
        "# Create a dataset instance without any transforms to ensure raw image statistics\n",
        "temp_dataset = DefectDetectionDataset(train_dir, os.path.join(train_dir, \"_annotations.coco.json\"), transforms=None)\n",
        "mean, std = calculate_dataset_stats(temp_dataset)\n",
        "\n",
        "print(f\"Calculated mean: {mean}\")\n",
        "print(f\"Calculated std: {std}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "9877b503-01bf-475e-b9cf-75983c0c56d1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform Function"
      ],
      "metadata": {
        "id": "X8_f-8Unq06T"
      },
      "id": "X8_f-8Unq06T"
    },
    {
      "id": "64f11bc5-1c34-4055-b9ac-e69f7d08188a",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Lee Zhen Ting\n",
        "########################################\n",
        "'''\n",
        "\n",
        "def get_transform(aug=True):\n",
        "    \"\"\"\n",
        "    Returns an Albumentations transform pipeline for training or validation.\n",
        "\n",
        "    Args:\n",
        "        aug (bool): If True, returns a training transform with augmentations.\n",
        "                    If False, returns a validation transform (no augmentation).\n",
        "\n",
        "    Returns:\n",
        "        Albumentations Compose object\n",
        "    \"\"\"\n",
        "\n",
        "    if aug:\n",
        "        # Training pipeline:\n",
        "        # Resize → Apply augmentations → Normalize → Convert to tensor\n",
        "        return A.Compose(\n",
        "            [\n",
        "                A.Resize(**augmentation_hyperparameters['resize']),\n",
        "                A.ColorJitter(**augmentation_hyperparameters['jitter']),\n",
        "                A.Rotate(**augmentation_hyperparameters['rotate']),\n",
        "                A.HorizontalFlip(**augmentation_hyperparameters['hflip']),\n",
        "                A.VerticalFlip(**augmentation_hyperparameters['vflip']),\n",
        "                A.Normalize(mean=mean, std=std),\n",
        "                ToTensorV2(),\n",
        "            ],\n",
        "            bbox_params=A.BboxParams(\n",
        "                format='pascal_voc',\n",
        "                label_fields=['labels'],\n",
        "                clip=True,\n",
        "                min_visibility=0.3\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Validation pipeline:\n",
        "    # Resize → Normalize → Convert to tensor\n",
        "    return A.Compose(\n",
        "        [\n",
        "            A.Resize(**augmentation_hyperparameters['resize']),\n",
        "            A.Normalize(mean=mean, std=std),\n",
        "            ToTensorV2(),\n",
        "        ],\n",
        "        bbox_params=A.BboxParams(\n",
        "            format='pascal_voc',\n",
        "            label_fields=['labels'],\n",
        "            clip=True,\n",
        "            min_visibility=0.3\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "id": "64f11bc5-1c34-4055-b9ac-e69f7d08188a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7fe0003b-0495-492a-aec6-1e1082ec1b23",
      "cell_type": "markdown",
      "source": [
        "## Load Datasets and DataLoaders"
      ],
      "metadata": {
        "id": "7fe0003b-0495-492a-aec6-1e1082ec1b23"
      }
    },
    {
      "id": "70c4520b-199e-422a-b67b-31384c4b2184",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Lee Zhen Ting\n",
        "########################################\n",
        "'''\n",
        "\n",
        "# True to apply augmentation on train dataset, otherwise false\n",
        "use_augmentation = True\n",
        "\n",
        "if use_augmentation:\n",
        "    train_dataset = DefectDetectionDataset(\n",
        "        train_dir,\n",
        "        os.path.join(train_dir, \"_annotations.coco.json\"),\n",
        "        transforms=get_transform(aug=True)\n",
        "    )\n",
        "else:\n",
        "    train_dataset = DefectDetectionDataset(\n",
        "        train_dir,\n",
        "        os.path.join(train_dir, \"_annotations.coco.json\"),\n",
        "        transforms=get_transform(aug=False)\n",
        "    )\n",
        "\n",
        "valid_dataset = DefectDetectionDataset(valid_dir, os.path.join(valid_dir, \"_annotations.coco.json\"), transforms=get_transform(aug=False))\n",
        "test_dataset  = DefectDetectionDataset(test_dir,  os.path.join(test_dir, \"_annotations.coco.json\"), transforms=get_transform(aug=False))\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size_valid, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "print(f\"Train Samples: {len(train_dataset)}, Valid Samples: {len(valid_dataset)}, Test Samples: {len(test_dataset)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "70c4520b-199e-422a-b67b-31384c4b2184"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "502db25f-28f5-4859-a5d9-69a0ef42b407",
      "cell_type": "markdown",
      "source": [
        "# 4. Training and Evaluation Functions"
      ],
      "metadata": {
        "id": "502db25f-28f5-4859-a5d9-69a0ef42b407"
      }
    },
    {
      "id": "e7087fb9",
      "cell_type": "markdown",
      "source": [
        "## Training Function"
      ],
      "metadata": {
        "id": "e7087fb9"
      }
    },
    {
      "id": "22a942a4-c137-48a2-bff1-d86787d58998",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Adapted from practical class materials\n",
        "########################################\n",
        "'''\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50):\n",
        "    \"\"\"\n",
        "    Train the object detection model for one epoch.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model (e.g., Faster R-CNN).\n",
        "        optimizer: Optimizer used for updating model weights.\n",
        "        data_loader: DataLoader providing training data.\n",
        "        device: Device to train on (CPU or CUDA).\n",
        "        epoch: Current epoch index (for logging).\n",
        "        print_freq: How often to print progress.\n",
        "\n",
        "    Returns:\n",
        "        float: Average training loss over the epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += losses.item()\n",
        "\n",
        "        if (batch_idx + 1) % print_freq == 0:\n",
        "            print(f\"[Epoch {epoch+1}] [Batch {batch_idx+1}/{len(data_loader)}] Training...\")\n",
        "\n",
        "    return running_loss / len(data_loader)"
      ],
      "metadata": {
        "trusted": true,
        "id": "22a942a4-c137-48a2-bff1-d86787d58998"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "65807c52-2915-4315-b05d-2a7c0e67012e",
      "cell_type": "markdown",
      "source": [
        "## Evaluation Function"
      ],
      "metadata": {
        "id": "65807c52-2915-4315-b05d-2a7c0e67012e"
      }
    },
    {
      "id": "b82f1376",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Goh Ken How\n",
        "########################################\n",
        "'''\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, device, epoch=None, print_freq=50):\n",
        "    \"\"\"\n",
        "    Evaluate the model on a validation set by computing loss.\n",
        "    Note: model.train() is used intentionally to enable loss computation during evaluation,\n",
        "    as detection models like Faster R-CNN only return losses in training mode.\n",
        "\n",
        "    Args:\n",
        "        model: The object detection model.\n",
        "        data_loader: DataLoader for the validation set.\n",
        "        device: Computation device (CPU or CUDA).\n",
        "        epoch: Current epoch (optional, for logging).\n",
        "        print_freq: How often to log batch progress.\n",
        "\n",
        "    Returns:\n",
        "        float: Average validation loss for the epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Required to compute loss in models\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        running_loss += losses.item()\n",
        "\n",
        "        if (batch_idx + 1) % print_freq == 0:\n",
        "            if epoch is not None:\n",
        "                print(f\"[Epoch {epoch+1}] [Batch {batch_idx+1}/{len(data_loader)}] Validating...\")\n",
        "\n",
        "    return running_loss / len(data_loader)"
      ],
      "metadata": {
        "trusted": true,
        "id": "b82f1376"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ec852e5f-b295-49c1-b151-d8db445aefac",
      "cell_type": "markdown",
      "source": [
        "## Calculate IOU Function"
      ],
      "metadata": {
        "id": "ec852e5f-b295-49c1-b151-d8db445aefac"
      }
    },
    {
      "id": "e8007dd2-bf5d-46f4-b5cb-797e01758732",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Lim Chia Yoong\n",
        "########################################\n",
        "'''\n",
        "\n",
        "def calculate_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Calculates the Intersection over Union (IoU) between two bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        box1: A list or tuple of format [xmin, ymin, xmax, ymax]\n",
        "        box2: A list or tuple of format [xmin, ymin, xmax, ymax]\n",
        "\n",
        "    Returns:\n",
        "        float: IoU value between 0 and 1\n",
        "    \"\"\"\n",
        "\n",
        "    x1, y1 = max(box1[0], box2[0]), max(box1[1], box2[1])\n",
        "    x2, y2 = min(box1[2], box2[2]), min(box1[3], box2[3])\n",
        "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    union_area = box1_area + box2_area - inter_area\n",
        "    if union_area == 0:\n",
        "        return 0\n",
        "    return inter_area / union_area"
      ],
      "metadata": {
        "trusted": true,
        "id": "e8007dd2-bf5d-46f4-b5cb-797e01758732"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bca834c9-17f8-4218-9aeb-6a64724c013a",
      "cell_type": "markdown",
      "source": [
        "## Calculate Precision-Recall Function"
      ],
      "metadata": {
        "id": "bca834c9-17f8-4218-9aeb-6a64724c013a"
      }
    },
    {
      "id": "ca2f89c3-647f-4bb3-ae36-ac5c57335db4",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Lim Chia Yoong\n",
        "########################################\n",
        "'''\n",
        "\n",
        "def calculate_precision_recall(scores, matches):\n",
        "    \"\"\"\n",
        "    Computes precision and recall values for a list of detection scores and match flags.\n",
        "\n",
        "    Args:\n",
        "        scores (list or array): Detection confidence scores.\n",
        "        matches (list or array): Match indicators (1 for true positive, 0 for false positive).\n",
        "\n",
        "    Returns:\n",
        "        precision (np.array): Precision values at each threshold.\n",
        "        recall (np.array): Recall values at each threshold.\n",
        "    \"\"\"\n",
        "\n",
        "    if len(scores) == 0:\n",
        "        return np.array([0]), np.array([0])\n",
        "\n",
        "    scores = np.array(scores)\n",
        "    matches = np.array(matches)\n",
        "\n",
        "    indices = np.argsort(-scores)\n",
        "    matches = matches[indices]\n",
        "\n",
        "    tp = np.cumsum(matches)\n",
        "    fp = np.cumsum(1 - matches)\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-6)\n",
        "    recall = tp / (tp[-1] + 1e-6)\n",
        "\n",
        "    return precision, recall"
      ],
      "metadata": {
        "trusted": true,
        "id": "ca2f89c3-647f-4bb3-ae36-ac5c57335db4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8cf3f43f-305a-4c98-aab7-6c99beeac16e",
      "cell_type": "markdown",
      "source": [
        "## Calculate mAP Function"
      ],
      "metadata": {
        "id": "8cf3f43f-305a-4c98-aab7-6c99beeac16e"
      }
    },
    {
      "id": "f28edd6d-a1f3-413b-9341-202236081ac9",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Goh Ken How\n",
        "########################################\n",
        "'''\n",
        "\n",
        "def calculate_map(scores, matches):\n",
        "    \"\"\"\n",
        "    Computes mean Average Precision (mAP) using 11-point interpolation.\n",
        "\n",
        "    Args:\n",
        "        scores (list or array): Detection confidence scores.\n",
        "        matches (list or array): Match indicators (1 = TP, 0 = FP).\n",
        "\n",
        "    Returns:\n",
        "        float: Average precision (AP) over 11 recall thresholds.\n",
        "    \"\"\"\n",
        "\n",
        "    precision, recall = calculate_precision_recall(scores, matches)\n",
        "    ap = 0\n",
        "    for t in np.linspace(0, 1, 11):\n",
        "        p = precision[recall >= t]\n",
        "        if p.size > 0:\n",
        "            ap += np.max(p)\n",
        "    return ap / 11"
      ],
      "metadata": {
        "trusted": true,
        "id": "f28edd6d-a1f3-413b-9341-202236081ac9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "197164e5-70a7-45b1-b4e8-3c58cfaa6357",
      "cell_type": "markdown",
      "source": [
        "## Compute mAP and PR Curves Function"
      ],
      "metadata": {
        "id": "197164e5-70a7-45b1-b4e8-3c58cfaa6357"
      }
    },
    {
      "id": "4ba4fe57-5072-4bea-af39-5d9f45c11bb8",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Goh Ken How\n",
        "########################################\n",
        "'''\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_map_and_pr_curves(model, data_loader, device, iou_threshold=iou_threshold):\n",
        "    \"\"\"\n",
        "    Computes global and per-class mAP and precision-recall curves for a given object detection model.\n",
        "\n",
        "    Args:\n",
        "        model: Trained object detection model.\n",
        "        data_loader: PyTorch DataLoader for the evaluation dataset.\n",
        "        device: Computation device (e.g., 'cuda' or 'cpu').\n",
        "        iou_threshold: IoU threshold to consider a prediction as a true positive.\n",
        "\n",
        "    Returns:\n",
        "        global_map (float): Mean average precision across all predictions.\n",
        "        per_class_map (dict): mAP per class {class_id: ap}.\n",
        "        global_precision (np.array): Precision values for all detections.\n",
        "        global_recall (np.array): Recall values for all detections.\n",
        "        per_class_pr (dict): Precision and recall arrays per class {class_id: (precision, recall)}.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    all_scores = []\n",
        "    all_matches = []\n",
        "\n",
        "    # Initialize per-class tracking\n",
        "    per_class_scores = {c: [] for c in VALID_CLASSES.values()}\n",
        "    per_class_matches = {c: [] for c in VALID_CLASSES.values()}\n",
        "\n",
        "    for images, targets in data_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        outputs = model(images)\n",
        "\n",
        "        for output, target in zip(outputs, targets):\n",
        "            pred_boxes = output['boxes'].cpu().numpy()\n",
        "            pred_scores = output['scores'].cpu().numpy()\n",
        "            pred_labels = output['labels'].cpu().numpy()\n",
        "\n",
        "            gt_boxes = target['boxes'].numpy()\n",
        "            gt_labels = target['labels'].numpy()\n",
        "\n",
        "            # Filter out low-confidence predictions\n",
        "            keep = pred_scores >= 0.05\n",
        "            pred_boxes = pred_boxes[keep]\n",
        "            pred_scores = pred_scores[keep]\n",
        "            pred_labels = pred_labels[keep]\n",
        "\n",
        "            matched = np.zeros(len(pred_boxes))\n",
        "\n",
        "            # Global matching (any class)\n",
        "            for gt_idx, gt_box in enumerate(gt_boxes):\n",
        "                best_iou, best_idx = 0, -1\n",
        "                for pred_idx, pred_box in enumerate(pred_boxes):\n",
        "                    iou = calculate_iou(gt_box, pred_box)\n",
        "                    if iou > best_iou:\n",
        "                        best_iou = iou\n",
        "                        best_idx = pred_idx\n",
        "\n",
        "                if best_iou >= iou_threshold:\n",
        "                    matched[best_idx] = 1\n",
        "\n",
        "            all_scores.extend(pred_scores.tolist())\n",
        "            all_matches.extend(matched.tolist())\n",
        "\n",
        "            # Per-class matching\n",
        "            for c in VALID_CLASSES.values():\n",
        "                gt_class_boxes = gt_boxes[gt_labels == c]\n",
        "                pred_class_boxes = pred_boxes[pred_labels == c]\n",
        "                pred_class_scores = pred_scores[pred_labels == c]\n",
        "\n",
        "                matched_c = np.zeros(len(pred_class_boxes))\n",
        "\n",
        "                for gt_idx, gt_box in enumerate(gt_class_boxes):\n",
        "                    best_iou_c, best_idx_c = 0, -1\n",
        "                    for pred_idx, pred_box in enumerate(pred_class_boxes):\n",
        "                        iou = calculate_iou(gt_box, pred_box)\n",
        "                        if iou > best_iou_c:\n",
        "                            best_iou_c = iou\n",
        "                            best_idx_c = pred_idx\n",
        "\n",
        "                    if best_iou_c >= iou_threshold:\n",
        "                        if best_idx_c != -1:\n",
        "                            matched_c[best_idx_c] = 1\n",
        "\n",
        "                per_class_scores[c].extend(pred_class_scores.tolist())\n",
        "                per_class_matches[c].extend(matched_c.tolist())\n",
        "\n",
        "    # Global precision-recall and mAP\n",
        "    global_precision, global_recall = calculate_precision_recall(all_scores, all_matches)\n",
        "    global_map = calculate_map(all_scores, all_matches)\n",
        "\n",
        "    # Per-class precision-recall and mAP\n",
        "    per_class_pr = {}\n",
        "    per_class_map = {}\n",
        "    for c in VALID_CLASSES.values():\n",
        "        p, r = calculate_precision_recall(per_class_scores[c], per_class_matches[c])\n",
        "        per_class_pr[c] = (p, r)\n",
        "        per_class_map[c] = calculate_map(per_class_scores[c], per_class_matches[c])\n",
        "\n",
        "    return global_map, per_class_map, global_precision, global_recall, per_class_pr"
      ],
      "metadata": {
        "trusted": true,
        "id": "4ba4fe57-5072-4bea-af39-5d9f45c11bb8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d392e622-b168-4d15-8185-5826e8e0eeb6",
      "cell_type": "markdown",
      "source": [
        "# 5. Model Training"
      ],
      "metadata": {
        "id": "d392e622-b168-4d15-8185-5826e8e0eeb6"
      }
    },
    {
      "id": "fa51bf3a-48f5-4d01-8190-ba1e8a9f3184",
      "cell_type": "markdown",
      "source": [
        "## Model Preparation"
      ],
      "metadata": {
        "id": "fa51bf3a-48f5-4d01-8190-ba1e8a9f3184"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom ROI Head with Dropout for Faster R-CNN"
      ],
      "metadata": {
        "id": "-wcTFY7yqi0Q"
      },
      "id": "-wcTFY7yqi0Q"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Lim Chia Yoong\n",
        "########################################\n",
        "'''\n",
        "\n",
        "class FastRCNNPredictorWithDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom classification and bounding box regression head for Faster R-CNN\n",
        "    with a dropout layer added for regularization.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of input features.\n",
        "        num_classes (int): Number of target classes (including background).\n",
        "        dropout_prob (float): Dropout probability to apply before prediction.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, num_classes, dropout_prob):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.cls_score = nn.Linear(in_channels, num_classes)\n",
        "        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the dropout and two prediction heads.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Feature vector from ROI pooling (shape: [N, in_channels]).\n",
        "\n",
        "        Returns:\n",
        "            scores (Tensor): Class scores for each ROI.\n",
        "            bbox_deltas (Tensor): Bounding box regression values for each ROI.\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        scores = self.cls_score(x)\n",
        "        bbox_deltas = self.bbox_pred(x)\n",
        "        return scores, bbox_deltas"
      ],
      "metadata": {
        "id": "JCNoAfOLn6O8"
      },
      "id": "JCNoAfOLn6O8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurable Faster R-CNN Loader with Optional Dropout and Fine-Tuning"
      ],
      "metadata": {
        "id": "FrRriK5GqlQO"
      },
      "id": "FrRriK5GqlQO"
    },
    {
      "id": "f1892566-187d-4692-a80e-42fa500e2e03",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Goh Ken How\n",
        "########################################\n",
        "'''\n",
        "\n",
        "def get_model(\n",
        "    num_classes,\n",
        "    weight_path=None,\n",
        "    use_dropout=False,\n",
        "    dropout_prob=0.5,\n",
        "    freeze_backbone=True,\n",
        "    unfreeze_layer=\"layer4\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Loads and customizes a Faster R-CNN model with options for dropout and fine-tuning.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): Number of classes including background.\n",
        "        weight_path (str, optional): Path to pretrained weights to load.\n",
        "        use_dropout (bool): Whether to use dropout in the predictor head.\n",
        "        dropout_prob (float): Dropout rate (if use_dropout is True).\n",
        "        freeze_backbone (bool): Whether to freeze all backbone layers initially.\n",
        "        unfreeze_layer (str): Name of layer to unfreeze (e.g., \"layer4\").\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): Configured Faster R-CNN model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load model (with or without pretrained COCO weights)\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "\n",
        "    # Replace the head to match custom class count\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    if use_dropout:\n",
        "        model.roi_heads.box_predictor = FastRCNNPredictorWithDropout(in_features, num_classes, dropout_prob)\n",
        "    else:\n",
        "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # Load custom-trained weights if provided\n",
        "    if weight_path:\n",
        "        model.load_state_dict(torch.load(weight_path, map_location=\"cpu\"))\n",
        "\n",
        "    # Optional: Freeze entire backbone\n",
        "    if freeze_backbone:\n",
        "        for param in model.backbone.body.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # Optional: Unfreeze a specific layer (e.g., 'layer4') in the backbone\n",
        "    if unfreeze_layer:\n",
        "        for name, param in model.backbone.body.named_parameters():\n",
        "            if unfreeze_layer in name:\n",
        "                param.requires_grad = True\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "id": "f1892566-187d-4692-a80e-42fa500e2e03"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Model"
      ],
      "metadata": {
        "id": "Xstw27wLqril"
      },
      "id": "Xstw27wLqril"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Goh Ken How\n",
        "########################################\n",
        "'''\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = get_model(num_classes=3)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "tV9-PWUjmyDo"
      },
      "id": "tV9-PWUjmyDo",
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "42d05716-b018-4f23-acd7-4625e32e864e",
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "42d05716-b018-4f23-acd7-4625e32e864e"
      }
    },
    {
      "id": "a637e57b-fdce-4438-8d03-2a2413ef0a50",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Lim Chia Yoong\n",
        "########################################\n",
        "'''\n",
        "\n",
        "# Initialize training state\n",
        "start_epoch = 0\n",
        "best_map = 0.0\n",
        "\n",
        "# Set up optimizer and learning rate scheduler\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "# Lists to store training history\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "maps = []\n",
        "per_class_maps = {c: [] for c in VALID_CLASSES.values()}\n",
        "\n",
        "# Load from checkpoint if available\n",
        "checkpoint_path = \"checkpoint.pth\"\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    best_map = checkpoint['best_map']\n",
        "    model.train()\n",
        "    print(f\"Loaded checkpoint from epoch {start_epoch-1}, best validation mAP so far: {best_map:.4f}\")\n",
        "else:\n",
        "    print(\"No checkpoint found, starting from scratch.\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
        "    valid_loss = evaluate(model, valid_loader, device, epoch)\n",
        "\n",
        "    global_map, per_class_map, global_precision, global_recall, per_class_pr = compute_map_and_pr_curves(model, valid_loader, device)\n",
        "\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    maps.append(global_map)\n",
        "    for c in VALID_CLASSES.values():\n",
        "        per_class_maps[c].append(per_class_map[c])\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Completed:\")\n",
        "    print(f\"    Train Loss        : {train_loss:.4f}\")\n",
        "    print(f\"    Valid Loss        : {valid_loss:.4f}\")\n",
        "    print(f\"    Validation mAP@0.5: {global_map:.4f}\")\n",
        "    print(f\"    Time Elapsed      : {elapsed/60:.2f} min\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Save best model\n",
        "    if global_map > best_map:\n",
        "        best_map = global_map\n",
        "        torch.save(model.state_dict(), \"best_fasterrcnn_defect_detection.pth\")\n",
        "        print(f\"Best model saved with mAP {best_map:.4f}\")\n",
        "\n",
        "    # Save full checkpoint\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': lr_scheduler.state_dict(),\n",
        "        'best_map': best_map\n",
        "    }, f'checkpoint_epoch{epoch+1}.pth')\n",
        "    print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
        "\n",
        "print(\"Training Complete!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "a637e57b-fdce-4438-8d03-2a2413ef0a50"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fa999eb3-1c6d-4ff8-86db-e6faf0ffab08",
      "cell_type": "markdown",
      "source": [
        "# 6. Metrics Saving and Visualizations"
      ],
      "metadata": {
        "id": "fa999eb3-1c6d-4ff8-86db-e6faf0ffab08"
      }
    },
    {
      "id": "2fd45480-b569-4c25-a078-2e5f4305918c",
      "cell_type": "markdown",
      "source": [
        "## Save Training Results to CSV"
      ],
      "metadata": {
        "id": "2fd45480-b569-4c25-a078-2e5f4305918c"
      }
    },
    {
      "id": "bef48c72-89cc-4f3f-9425-3fcb2db46484",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Lim Chia Yoong\n",
        "########################################\n",
        "'''\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    \"Epoch\": list(range(1, num_epochs+1)),\n",
        "    \"Train Loss\": train_losses,\n",
        "    \"Valid Loss\": valid_losses,\n",
        "    \"mAP@0.5\": maps\n",
        "})\n",
        "metrics_df.to_csv(\"training_metrics.csv\", index=False)\n",
        "print(\"Saved training metrics to training_metrics.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "bef48c72-89cc-4f3f-9425-3fcb2db46484"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fd26c918-f9e6-4878-89d9-1c4cec75037b",
      "cell_type": "markdown",
      "source": [
        "## Plot Loss Curves"
      ],
      "metadata": {
        "id": "fd26c918-f9e6-4878-89d9-1c4cec75037b"
      }
    },
    {
      "id": "fcb8a25e-8bd6-48d0-96b2-3b977a6bdda3",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Goh Ken How\n",
        "########################################\n",
        "'''\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(valid_losses, label=\"Valid Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Train and Validation Loss\")\n",
        "plt.grid(True)\n",
        "plt.savefig(\"loss_curve.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "fcb8a25e-8bd6-48d0-96b2-3b977a6bdda3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "73314002-29e9-481a-ba0f-44365c23a23f",
      "cell_type": "markdown",
      "source": [
        "## Plot Overall mAP Curve"
      ],
      "metadata": {
        "id": "73314002-29e9-481a-ba0f-44365c23a23f"
      }
    },
    {
      "id": "65b6c17a-903e-4da3-8830-692a2f9a2dea",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Lee Zhen Ting\n",
        "########################################\n",
        "'''\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(maps, label=\"Validation mAP@0.5\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"mAP@0.5\")\n",
        "plt.legend()\n",
        "plt.title(\"Validation mAP@0.5 Curve\")\n",
        "plt.grid(True)\n",
        "plt.savefig(\"map_curve.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "65b6c17a-903e-4da3-8830-692a2f9a2dea"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "337c0ebb-4682-4020-806a-826b0f33e7af",
      "cell_type": "markdown",
      "source": [
        "## Plot Per-Class mAP Curve"
      ],
      "metadata": {
        "id": "337c0ebb-4682-4020-806a-826b0f33e7af"
      }
    },
    {
      "id": "dff0755d-656a-48df-92ff-a58a1769b920",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Lee Zhen Ting\n",
        "########################################\n",
        "'''\n",
        "\n",
        "plt.figure()\n",
        "for c, values in per_class_maps.items():\n",
        "    class_name = [k for k,v in VALID_CLASSES.items() if v == c][0]\n",
        "    plt.plot(values, label=f\"{class_name}\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Per-Class mAP@0.5\")\n",
        "plt.legend()\n",
        "plt.title(\"Per-Class Validation mAP@0.5\")\n",
        "plt.grid(True)\n",
        "plt.savefig(\"per_class_map_curve.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "dff0755d-656a-48df-92ff-a58a1769b920"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "003d0c23-646c-4178-8604-28bdffbbccde",
      "cell_type": "markdown",
      "source": [
        "## Plot PR Curves"
      ],
      "metadata": {
        "id": "003d0c23-646c-4178-8604-28bdffbbccde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot PR Curves Function"
      ],
      "metadata": {
        "id": "2NV3qnnPri2o"
      },
      "id": "2NV3qnnPri2o"
    },
    {
      "id": "bca0ace2-4069-4414-89c9-e785f6bba010",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Lim Chia Yoong\n",
        "########################################\n",
        "'''\n",
        "\n",
        "def plot_pr_curve(precision, recall, title, filename):\n",
        "    plt.figure()\n",
        "    plt.plot(recall, precision, marker='.')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.savefig(filename)\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "bca0ace2-4069-4414-89c9-e785f6bba010"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Global PR Curve"
      ],
      "metadata": {
        "id": "yX3Qk1sMrnDd"
      },
      "id": "yX3Qk1sMrnDd"
    },
    {
      "id": "339e6d94-6cc9-4e17-b179-30cebe544da5",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Lim Chia Yoong\n",
        "########################################\n",
        "'''\n",
        "\n",
        "plot_pr_curve(global_precision, global_recall, \"Global PR Curve\", \"global_pr_curve.png\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "339e6d94-6cc9-4e17-b179-30cebe544da5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Per-Class PR Curves"
      ],
      "metadata": {
        "id": "DnAoO2wurszK"
      },
      "id": "DnAoO2wurszK"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Lim Chia Yoong\n",
        "########################################\n",
        "'''\n",
        "\n",
        "for class_id, (precision_c, recall_c) in per_class_pr.items():\n",
        "    class_name = [k for k,v in VALID_CLASSES.items() if v == class_id][0]\n",
        "    plot_pr_curve(precision_c, recall_c, f\"{class_name} PR Curve\", f\"{class_name.lower()}_pr_curve.png\")"
      ],
      "metadata": {
        "id": "HWiQBoSerp3V"
      },
      "id": "HWiQBoSerp3V",
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "ce474f7d-e323-4d30-8c8e-ea781fae4b0b",
      "cell_type": "markdown",
      "source": [
        "## Visualize Predictions"
      ],
      "metadata": {
        "id": "ce474f7d-e323-4d30-8c8e-ea781fae4b0b"
      }
    },
    {
      "id": "95ae11bc-0440-4d2c-b939-4bb0b689201f",
      "cell_type": "code",
      "source": [
        "'''\n",
        "########################################\n",
        "Coded by: Goh Ken How\n",
        "########################################\n",
        "'''\n",
        "\n",
        "model.eval()\n",
        "for i in range(5):\n",
        "    img, _ = test_dataset[i]\n",
        "    img = img.to(device)\n",
        "    with torch.no_grad():\n",
        "        prediction = model([img])\n",
        "\n",
        "    img = img.permute(1,2,0).cpu().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12,9))\n",
        "    ax.imshow(img)\n",
        "    boxes = prediction[0]['boxes'].cpu().numpy()\n",
        "    scores = prediction[0]['scores'].cpu().numpy()\n",
        "    labels = prediction[0]['labels'].cpu().numpy()\n",
        "\n",
        "    for box, score, label in zip(boxes, scores, labels):\n",
        "        if score >= score_threshold:\n",
        "            xmin, ymin, xmax, ymax = box\n",
        "            width, height = xmax - xmin, ymax - ymin\n",
        "            rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            label_name = [k for k,v in VALID_CLASSES.items() if v == label][0]\n",
        "            ax.text(xmin, ymin, f\"{label_name}: {score:.2f}\", color='white', fontsize=12, backgroundcolor='red')\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.savefig(f\"prediction_{i}.png\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "95ae11bc-0440-4d2c-b939-4bb0b689201f"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}